{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Customized version of the official evaluation script for v1.1 of the SQuAD dataset. \"\"\"\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        #b 称为单词边界（word boundary）符.例如只想匹配 My cat is bad.中的cat 可以使用 \\bcat\\b\n",
    "        #注意\\b在python中是回退符，正则模式之前一定要加r(raw string)\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):#不规则的空格都换成固定的空格\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)#标点符号\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):#都是单条prediction和单条ground_truth\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()#ground_truth 真实值，标签\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)#还是有共同的key的Counter，不过value取少的那个\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):#都是单条prediction和单条ground_truth\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(dataset, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    message = 'Unanswered question ' + qa['id'] + \\\n",
    "                              ' will receive score 0.'\n",
    "                    print(message, file=sys.stderr)#会红色字体提示错误\n",
    "                    continue\n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                prediction = predictions[qa['id']]#why ?\n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    with open(args.dataset_file) as dataset_file:\n",
    "        dataset_json = json.load(dataset_file)\n",
    "        \"\"\"\n",
    "        if (dataset_json['version'] != expected_version):\n",
    "            print('Evaluation expects v-' + expected_version +\n",
    "                  ', but got dataset with v-' + dataset_json['version'],\n",
    "                  file=sys.stderr)\n",
    "        \"\"\"\n",
    "        dataset = dataset_json['data']\n",
    "    with open(args.prediction_file) as prediction_file:\n",
    "        predictions = json.load(prediction_file)\n",
    "\n",
    "    results = evaluate(dataset, predictions)\n",
    "    #print(json.dumps(results))\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    expected_version = '1.1'\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Evaluation for SQuAD ' + expected_version)\n",
    "    parser.add_argument('dataset_file', help='Dataset file')\n",
    "    parser.add_argument('prediction_file', help='Prediction File')\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
