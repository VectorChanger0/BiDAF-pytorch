{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.nn import LSTM, Linear\n",
    "\n",
    "\n",
    "class BiDAF(nn.Module):\n",
    "    def __init__(self, args, pretrained):\n",
    "        super(BiDAF, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        # 1. Character Embedding Layer\n",
    "        self.char_emb = nn.Embedding(args.char_vocab_size, args.char_dim, padding_idx=1)\n",
    "        nn.init.uniform_(self.char_emb.weight, -0.001, 0.001)\n",
    "\n",
    "        self.char_conv = nn.Conv2d(1, args.char_channel_size, (args.char_dim, args.char_channel_width))\n",
    "\n",
    "        # 2. Word Embedding Layer\n",
    "        # initialize word embedding with GloVe\n",
    "        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=True)\n",
    "\n",
    "        # highway network\n",
    "        assert self.args.hidden_size * 2 == (self.args.char_channel_size + self.args.word_dim)\n",
    "        for i in range(2):\n",
    "            setattr(self, f'highway_linear{i}',\n",
    "                    nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2),\n",
    "                                  nn.ReLU()))\n",
    "            setattr(self, f'highway_gate{i}',\n",
    "                    nn.Sequential(Linear(args.hidden_size * 2, args.hidden_size * 2),\n",
    "                                  nn.Sigmoid()))\n",
    "\n",
    "        # 3. Contextual Embedding Layer\n",
    "        self.context_LSTM = LSTM(input_size=args.hidden_size * 2,\n",
    "                                 hidden_size=args.hidden_size,\n",
    "                                 bidirectional=True,\n",
    "                                 batch_first=True,\n",
    "                                 dropout=args.dropout)\n",
    "\n",
    "        # 4. Attention Flow Layer\n",
    "        self.att_weight_c = Linear(args.hidden_size * 2, 1)\n",
    "        self.att_weight_q = Linear(args.hidden_size * 2, 1)\n",
    "        self.att_weight_cq = Linear(args.hidden_size * 2, 1)\n",
    "\n",
    "        # 5. Modeling Layer\n",
    "        self.modeling_LSTM1 = LSTM(input_size=args.hidden_size * 8,\n",
    "                                   hidden_size=args.hidden_size,\n",
    "                                   bidirectional=True,\n",
    "                                   batch_first=True,\n",
    "                                   dropout=args.dropout)\n",
    "\n",
    "        self.modeling_LSTM2 = LSTM(input_size=args.hidden_size * 2,\n",
    "                                   hidden_size=args.hidden_size,\n",
    "                                   bidirectional=True,\n",
    "                                   batch_first=True,\n",
    "                                   dropout=args.dropout)\n",
    "\n",
    "        # 6. Output Layer\n",
    "        self.p1_weight_g = Linear(args.hidden_size * 8, 1, dropout=args.dropout)\n",
    "        self.p1_weight_m = Linear(args.hidden_size * 2, 1, dropout=args.dropout)\n",
    "        self.p2_weight_g = Linear(args.hidden_size * 8, 1, dropout=args.dropout)\n",
    "        self.p2_weight_m = Linear(args.hidden_size * 2, 1, dropout=args.dropout)\n",
    "\n",
    "        self.output_LSTM = LSTM(input_size=args.hidden_size * 2,\n",
    "                                hidden_size=args.hidden_size,\n",
    "                                bidirectional=True,\n",
    "                                batch_first=True,\n",
    "                                dropout=args.dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=args.dropout)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # TODO: More memory-efficient architecture\n",
    "        def char_emb_layer(x):\n",
    "            \"\"\"\n",
    "            :param x: (batch, seq_len, word_len)\n",
    "            :return: (batch, seq_len, char_channel_size)\n",
    "            \"\"\"\n",
    "            batch_size = x.size(0)\n",
    "            # (batch, seq_len, word_len, char_dim)\n",
    "            x = self.dropout(self.char_emb(x))\n",
    "            # (batch * seq_len, 1, char_dim, word_len)\n",
    "            x = x.view(-1, self.args.char_dim, x.size(2)).unsqueeze(1)\n",
    "            # (batch * seq_len, char_channel_size, 1, conv_len) -> (batch * seq_len, char_channel_size, conv_len)\n",
    "            x = self.char_conv(x).squeeze()\n",
    "            # (batch * seq_len, char_channel_size, 1) -> (batch * seq_len, char_channel_size)\n",
    "            x = F.max_pool1d(x, x.size(2)).squeeze()\n",
    "            # (batch, seq_len, char_channel_size)\n",
    "            x = x.view(batch_size, -1, self.args.char_channel_size)\n",
    "\n",
    "            return x\n",
    "\n",
    "        def highway_network(x1, x2):\n",
    "            \"\"\"\n",
    "            :param x1: (batch, seq_len, char_channel_size)\n",
    "            :param x2: (batch, seq_len, word_dim)\n",
    "            :return: (batch, seq_len, hidden_size * 2)\n",
    "            \"\"\"\n",
    "            # (batch, seq_len, char_channel_size + word_dim)\n",
    "            x = torch.cat([x1, x2], dim=-1)\n",
    "            for i in range(2):\n",
    "                h = getattr(self, f'highway_linear{i}')(x)\n",
    "                g = getattr(self, f'highway_gate{i}')(x)\n",
    "                x = g * h + (1 - g) * x\n",
    "            # (batch, seq_len, hidden_size * 2)\n",
    "            return x\n",
    "\n",
    "        def att_flow_layer(c, q):\n",
    "            \"\"\"\n",
    "            :param c: (batch, c_len, hidden_size * 2)\n",
    "            :param q: (batch, q_len, hidden_size * 2)\n",
    "            :return: (batch, c_len, q_len)\n",
    "            \"\"\"\n",
    "            c_len = c.size(1)\n",
    "            q_len = q.size(1)\n",
    "\n",
    "            # (batch, c_len, q_len, hidden_size * 2)\n",
    "            #c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)\n",
    "            # (batch, c_len, q_len, hidden_size * 2)\n",
    "            #q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
    "            # (batch, c_len, q_len, hidden_size * 2)\n",
    "            #cq_tiled = c_tiled * q_tiled\n",
    "            #cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
    "\n",
    "            cq = []\n",
    "            for i in range(q_len):\n",
    "                #(batch, 1, hidden_size * 2)\n",
    "                qi = q.select(1, i).unsqueeze(1)\n",
    "                #(batch, c_len, 1)\n",
    "                ci = self.att_weight_cq(c * qi).squeeze()\n",
    "                cq.append(ci)\n",
    "            # (batch, c_len, q_len)\n",
    "            cq = torch.stack(cq, dim=-1)\n",
    "\n",
    "            # (batch, c_len, q_len)\n",
    "            s = self.att_weight_c(c).expand(-1, -1, q_len) + \\\n",
    "                self.att_weight_q(q).permute(0, 2, 1).expand(-1, c_len, -1) + \\\n",
    "                cq\n",
    "\n",
    "            # (batch, c_len, q_len)\n",
    "            a = F.softmax(s, dim=2)\n",
    "            # (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -> (batch, c_len, hidden_size * 2)\n",
    "            c2q_att = torch.bmm(a, q)\n",
    "            # (batch, 1, c_len)\n",
    "            b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
    "            # (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -> (batch, hidden_size * 2)\n",
    "            q2c_att = torch.bmm(b, c).squeeze()\n",
    "            # (batch, c_len, hidden_size * 2) (tiled)\n",
    "            q2c_att = q2c_att.unsqueeze(1).expand(-1, c_len, -1)\n",
    "            # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
    "\n",
    "            # (batch, c_len, hidden_size * 8)\n",
    "            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=-1)\n",
    "            return x\n",
    "\n",
    "        def output_layer(g, m, l):\n",
    "            \"\"\"\n",
    "            :param g: (batch, c_len, hidden_size * 8)\n",
    "            :param m: (batch, c_len ,hidden_size * 2)\n",
    "            :return: p1: (batch, c_len), p2: (batch, c_len)\n",
    "            \"\"\"\n",
    "            # (batch, c_len)\n",
    "            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()\n",
    "            # (batch, c_len, hidden_size * 2)\n",
    "            m2 = self.output_LSTM((m, l))[0]\n",
    "            # (batch, c_len)\n",
    "            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()\n",
    "\n",
    "            return p1, p2\n",
    "\n",
    "        # 1. Character Embedding Layer\n",
    "        c_char = char_emb_layer(batch.c_char)\n",
    "        q_char = char_emb_layer(batch.q_char)\n",
    "        # 2. Word Embedding Layer\n",
    "        c_word = self.word_emb(batch.c_word[0])\n",
    "        q_word = self.word_emb(batch.q_word[0])\n",
    "        c_lens = batch.c_word[1]\n",
    "        q_lens = batch.q_word[1]\n",
    "\n",
    "        # Highway network\n",
    "        c = highway_network(c_char, c_word)\n",
    "        q = highway_network(q_char, q_word)\n",
    "        # 3. Contextual Embedding Layer\n",
    "        c = self.context_LSTM((c, c_lens))[0]\n",
    "        q = self.context_LSTM((q, q_lens))[0]\n",
    "        # 4. Attention Flow Layer\n",
    "        g = att_flow_layer(c, q)\n",
    "        # 5. Modeling Layer\n",
    "        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[0], c_lens))[0]\n",
    "        # 6. Output Layer\n",
    "        p1, p2 = output_layer(g, m, c_lens)\n",
    "\n",
    "        # (batch, c_len), (batch, c_len)\n",
    "        return p1, p2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
