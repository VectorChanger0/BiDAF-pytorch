{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "\n",
    "def word_tokenize(tokens):\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "\n",
    "\n",
    "class SQuAD():\n",
    "    def __init__(self, args):\n",
    "        path = '.data/squad'\n",
    "        dataset_path = path + '/torchtext/'\n",
    "        train_examples_path = dataset_path + 'train_examples.pt'\n",
    "        dev_examples_path = dataset_path + 'dev_examples.pt'\n",
    "\n",
    "        print(\"preprocessing data files...\")\n",
    "        if not os.path.exists(f'{path}/{args.train_file}l'):\n",
    "            self.preprocess_file(f'{path}/{args.train_file}')\n",
    "        if not os.path.exists(f'{path}/{args.dev_file}l'):\n",
    "            self.preprocess_file(f'{path}/{args.dev_file}')\n",
    "\n",
    "        self.RAW = data.RawField()\n",
    "        self.CHAR_NESTING = data.Field(batch_first=True, tokenize=list, lower=True)#默认sequential会分词\n",
    "        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)\n",
    "        #A nested field holds another field (called *nesting field*), accepts an untokenized\n",
    "        # string or a list string tokens and groups and treats them as one field as described\n",
    "        # by the nesting field.\n",
    "        #传入了一个如何分词的函数\n",
    "        self.WORD = data.Field(batch_first=True, tokenize=word_tokenize, lower=True, include_lengths=True)\n",
    "        self.LABEL = data.Field(sequential=False, unk_token=None, use_vocab=False)\n",
    "\n",
    "        dict_fields = {'id': ('id', self.RAW),\n",
    "                       's_idx': ('s_idx', self.LABEL),\n",
    "                       'e_idx': ('e_idx', self.LABEL),\n",
    "                       'context': [('c_word', self.WORD), ('c_char', self.CHAR)],\n",
    "                       'question': [('q_word', self.WORD), ('q_char', self.CHAR)]}\n",
    "\n",
    "        list_fields = [('id', self.RAW), ('s_idx', self.LABEL), ('e_idx', self.LABEL),\n",
    "                       ('c_word', self.WORD), ('c_char', self.CHAR),\n",
    "                       ('q_word', self.WORD), ('q_char', self.CHAR)]\n",
    "\n",
    "        if os.path.exists(dataset_path):\n",
    "            print(\"loading splits...\")\n",
    "            train_examples = torch.load(train_examples_path)\n",
    "            dev_examples = torch.load(dev_examples_path)\n",
    "\n",
    "            self.train = data.Dataset(examples=train_examples, fields=list_fields)\n",
    "            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)\n",
    "        else:\n",
    "            print(\"building splits...\")\n",
    "            #第一次是进入else这里\n",
    "            #data.TabularDataset Defines a Dataset of columns stored in CSV, TSV, or JSON format\n",
    "            self.train, self.dev = data.TabularDataset.splits(\n",
    "                path=path,\n",
    "                train=f'{args.train_file}l',\n",
    "                validation=f'{args.dev_file}l',\n",
    "                format='json',\n",
    "                fields=dict_fields)\n",
    "\n",
    "            os.makedirs(dataset_path)\n",
    "            torch.save(self.train.examples, train_examples_path)\n",
    "            torch.save(self.dev.examples, dev_examples_path)\n",
    "\n",
    "        #cut too long context in the training set for efficiency.\n",
    "        if args.context_threshold > 0:\n",
    "            self.train.examples = [e for e in self.train.examples if len(e.c_word) <= args.context_threshold]\n",
    "\n",
    "        print(\"building vocab...\")\n",
    "        self.CHAR.build_vocab(self.train, self.dev)\n",
    "        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name='6B', dim=args.word_dim))\n",
    "\n",
    "        print(\"building iterators...\")\n",
    "        self.train_iter, self.dev_iter = \\\n",
    "            data.BucketIterator.splits((self.train, self.dev),\n",
    "                                       batch_sizes=[args.train_batch_size, args.dev_batch_size],\n",
    "                                       device=args.gpu,\n",
    "                                       sort_key=lambda x: len(x.c_word))\n",
    "\n",
    "    def preprocess_file(self, path):\n",
    "        dump = []\n",
    "        abnormals = [' ', '\\n', '\\u3000', '\\u202f', '\\u2009']\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data = data['data']\n",
    "\n",
    "            for article in data:\n",
    "                for paragraph in article['paragraphs']:\n",
    "                    context = paragraph['context']\n",
    "                    tokens = word_tokenize(context)\n",
    "                    for qa in paragraph['qas']:\n",
    "                        id = qa['id']\n",
    "                        question = qa['question']\n",
    "                        for ans in qa['answers']:\n",
    "                            answer = ans['text']\n",
    "                            s_idx = ans['answer_start']\n",
    "                            e_idx = s_idx + len(answer)\n",
    "\n",
    "                            l = 0\n",
    "                            s_found = False\n",
    "                            for i, t in enumerate(tokens):\n",
    "                                while l < len(context):\n",
    "                                    if context[l] in abnormals:\n",
    "                                        l += 1\n",
    "                                    else:\n",
    "                                        break\n",
    "                                # exceptional cases\n",
    "                                if t[0] == '\"' and context[l:l + 2] == '\\'\\'':\n",
    "                                    t = '\\'\\'' + t[1:]\n",
    "                                elif t == '\"' and context[l:l + 2] == '\\'\\'':\n",
    "                                    t = '\\'\\''\n",
    "\n",
    "                                l += len(t)\n",
    "                                if l > s_idx and s_found == False:\n",
    "                                    s_idx = i\n",
    "                                    s_found = True\n",
    "                                if l >= e_idx:\n",
    "                                    e_idx = i\n",
    "                                    break\n",
    "\n",
    "                            dump.append(dict([('id', id),\n",
    "                                              ('context', context),\n",
    "                                              ('question', question),\n",
    "                                              ('answer', answer),\n",
    "                                              ('s_idx', s_idx),\n",
    "                                              ('e_idx', e_idx)]))\n",
    "\n",
    "        with open(f'{path}l', 'w', encoding='utf-8') as f:\n",
    "            for line in dump:\n",
    "                json.dump(line, f)\n",
    "                print('', file=f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
