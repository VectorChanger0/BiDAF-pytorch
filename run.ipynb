{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy, json, os\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from model.model import BiDAF\n",
    "from model.data import SQuAD\n",
    "from model.ema import EMA\n",
    "import evaluate\n",
    "# torch.set_grad_enabled(False)\n",
    "\n",
    "def train(args, data):\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BiDAF(args, data.WORD.vocab.vectors).to(device)\n",
    "\n",
    "    ema = EMA(args.exp_decay_rate)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            ema.register(name, param.data)\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = optim.Adadelta(parameters, lr=args.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    writer = SummaryWriter(log_dir='runs/' + args.model_time)\n",
    "\n",
    "    model.train()\n",
    "    loss, last_epoch = 0, -1\n",
    "    max_dev_exact, max_dev_f1 = -1, -1\n",
    "\n",
    "    iterator = data.train_iter\n",
    "    for i, batch in enumerate(iterator):\n",
    "        present_epoch = int(iterator.epoch)\n",
    "        if present_epoch == args.epoch:\n",
    "            break\n",
    "        if present_epoch > last_epoch:\n",
    "            print('epoch:', present_epoch + 1)\n",
    "        last_epoch = present_epoch\n",
    "\n",
    "        p1, p2 = model(batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
    "        loss += batch_loss.item()#利用item得到loss的值\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                ema.update(name, param.data)\n",
    "\n",
    "        if (i + 1) % args.print_freq == 0:\n",
    "            dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)#选择data.dev_iter里面的数据\n",
    "            c = (i + 1) // args.print_freq\n",
    "\n",
    "            writer.add_scalar('loss/train', loss, c)\n",
    "            writer.add_scalar('loss/dev', dev_loss, c)\n",
    "            writer.add_scalar('exact_match/dev', dev_exact, c)\n",
    "            writer.add_scalar('f1/dev', dev_f1, c)\n",
    "            print(f'train loss: {loss:.3f} / dev loss: {dev_loss:.3f}'\n",
    "                  f' / dev EM: {dev_exact:.3f} / dev F1: {dev_f1:.3f}')\n",
    "\n",
    "            if dev_f1 > max_dev_f1:\n",
    "                max_dev_f1 = dev_f1\n",
    "                max_dev_exact = dev_exact\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            loss = 0\n",
    "            model.train()\n",
    "\n",
    "    writer.close()\n",
    "    print(f'max dev EM: {max_dev_exact:.3f} / max dev F1: {max_dev_f1:.3f}')\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def test(model, ema, args, data):\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = 0\n",
    "    answers = dict()\n",
    "    model.eval()\n",
    "\n",
    "    backup_params = EMA(0)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            backup_params.register(name, param.data)\n",
    "            param.data.copy_(ema.get(name))\n",
    "\n",
    "    for batch in iter(data.dev_iter):\n",
    "        p1, p2 = model(batch)\n",
    "        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # (batch, c_len, c_len)\n",
    "        batch_size, c_len = p1.size() # why c_len 的意思？\n",
    "        ls = nn.LogSoftmax(dim=1)\n",
    "        mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        #tril(-1)将矩阵主对角线以下1格的元素保留，其他全设置为0\n",
    "        score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask# why unsqueeze at different dim?\n",
    "        score, s_idx = score.max(dim=1)#??? why\n",
    "        score, e_idx = score.max(dim=1)#???\n",
    "        s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()#抽取\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            id = batch.id[i]\n",
    "            answer = batch.c_word[0][i][s_idx[i]:e_idx[i] + 1]\n",
    "            answer = ' '.join([data.WORD.vocab.itos[idx] for idx in answer])\n",
    "            answers[id] = answer\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data.copy_(backup_params.get(name))\n",
    "\n",
    "    with open(args.prediction_file, 'w', encoding='utf-8') as f:#先存dev的predictions文件，然后evaluate去和真实值比较\n",
    "        print(json.dumps(answers), file=f)\n",
    "\n",
    "    results = evaluate.main(args)\n",
    "    return loss, results['exact_match'], results['f1']\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--char-dim', default=8, type=int)\n",
    "    parser.add_argument('--char-channel-width', default=5, type=int)\n",
    "    parser.add_argument('--char-channel-size', default=100, type=int)\n",
    "    parser.add_argument('--context-threshold', default=300, type=int)#init 400\n",
    "    parser.add_argument('--dev-batch-size', default=100, type=int)\n",
    "    parser.add_argument('--dev-file', default='dev-v1.1.json')\n",
    "    parser.add_argument('--dropout', default=0.2, type=float)\n",
    "    parser.add_argument('--epoch', default=12, type=int)\n",
    "    parser.add_argument('--exp-decay-rate', default=0.999, type=float)\n",
    "    parser.add_argument('--gpu', default=0, type=int)\n",
    "    parser.add_argument('--hidden-size', default=100, type=int)\n",
    "    parser.add_argument('--learning-rate', default=0.5, type=float)\n",
    "    parser.add_argument('--print-freq', default=250, type=int)\n",
    "    parser.add_argument('--train-batch-size', default=3, type=int)# init 60\n",
    "    parser.add_argument('--train-file', default='train-v1.1.json')\n",
    "    parser.add_argument('--word-dim', default=100, type=int)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print('loading SQuAD data...')\n",
    "    data = SQuAD(args)\n",
    "    setattr(args, 'char_vocab_size', len(data.CHAR.vocab))\n",
    "    setattr(args, 'word_vocab_size', len(data.WORD.vocab))\n",
    "    setattr(args, 'dataset_file', f'.data/squad/{args.dev_file}')\n",
    "    setattr(args, 'prediction_file', f'prediction{args.gpu}.out')\n",
    "    # setattr(args, 'model_time', strftime('%H:%M:%S', gmtime()))#英国格林威治时间\n",
    "    #linux 下面文件可以带冒号，windows不可以\n",
    "    setattr(args, 'model_time', strftime('%H.%M.%S', gmtime()))#英国格林威治时间\n",
    "\n",
    "    print('data loading complete!')\n",
    "\n",
    "    print('training start!')\n",
    "    best_model = train(args, data)\n",
    "    if not os.path.exists('saved_models'):\n",
    "        os.makedirs('saved_models')\n",
    "    torch.save(best_model.state_dict(), f'saved_models/BiDAF_{args.model_time}.pt')\n",
    "    print('training finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
